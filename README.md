# ğŸ§  Modular RAG PDF Chatbot with FastAPI, ChromaDB & Streamlit



This project is a modular **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDF documents and chat with an AI assistant that answers queries based on the document content. It features a microservice architecture with a decoupled **FastAPI backend** and **Streamlit frontend**, using **ChromaDB** as the vector store and **Groq's LLaMA3 model** as the LLM.

---

## ğŸ“‚ Project Structure

```
ragbot2.0/
â”œâ”€â”€ client/         # Streamlit Frontend
â”‚   |â”€â”€components/
|   |  |â”€â”€chatUI.py
|   |  |â”€â”€history_download.py
|   |  |â”€â”€upload.py
|   |â”€â”€utils/
|   |  |â”€â”€api.py
|   |â”€â”€app.py
|   |â”€â”€config.py
â”œâ”€â”€ server/         # FastAPI Backend
â”‚   â”œâ”€â”€ chroma_store/ ....after run
|   |â”€â”€modules/
â”‚      â”œâ”€â”€ load_vectorestore.py
â”‚      â”œâ”€â”€ llm.py
â”‚      â”œâ”€â”€ pdf_handler.py
â”‚      â”œâ”€â”€ query_handlers.py
|   |â”€â”€uploaded_pdfs/ ....after run
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ main.py
â””â”€â”€ README.md
```

---

## âœ¨ Features

- ğŸ“„ Upload and parse PDFs
- ğŸ§  Embed document chunks with HuggingFace embeddings
- ğŸ’‚ï¸ Store embeddings in ChromaDB
- ğŸ’¬ Query documents using LLaMA3 via Groq
- ğŸŒ Microservice architecture (Streamlit client + FastAPI server)

---

## ğŸ“ How RAG Works

Retrieval-Augmented Generation (RAG) enhances LLMs by injecting external knowledge. Instead of relying solely on pre-trained data, the model retrieves relevant information from a vector database (like ChromaDB) and uses it to generate accurate, context-aware responses.

---

## ğŸ“Š Application Diagram

ğŸ“„ [Download the Full Architecture PDF](assets/ragbot2.0.pdf)

---

## ğŸš€ Getting Started Locally

### 1. Clone the Repository

```bash
git clone https://github.com/vatsal8103/Hackrx-6.0-code
cd RagBot-2.0
```

### 2. Setup the Backend (FastAPI)

```bash
cd server
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Set your Groq API Key (.env)
GROQ_API_KEY="your_key_here"

# Run the FastAPI server
uvicorn main:app --reload
```

### 3. Setup the Frontend (Streamlit)

```bash
cd ../client
pip install -r requirements.txt  # if you use a separate venv for client
streamlit run app.py
```

---

## ğŸŒ API Endpoints (FastAPI)

- `POST /upload_pdfs/` â€” Upload PDFs and build vectorstore
- `POST /ask/` â€” Send a query and receive answers

Testable via Postman or directly from the Streamlit frontend.

---

## ğŸš§ TODO

- [ ] Add authentication for endpoints
- [ ] Dockerize the project
- [ ] Add support for more file types

---

## ğŸŒŸ Credits

- [LangChain](https://www.langchain.com/)
- [ChromaDB](https://www.trychroma.com/)
- [Groq](https://groq.com/)
- [Streamlit](https://streamlit.io/)

---

## âœ‰ï¸ Contact

For questions or suggestions, open an issue or contact at [vatsalchauhan8103@gmail.com]

---

> Happy Building RAGbots! ğŸš€
